{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2570276-eba4-4217-9dae-675b57b4125a",
   "metadata": {},
   "source": [
    "## 1. Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c10dd-c29d-4991-b657-b446be774362",
   "metadata": {},
   "source": [
    "- A loss function or cost function $L[φ]$ returns a single number that describes the mismatch between the model predictions $f[x_{i}, φ]$ and their corresponding ground-truth outputs $y_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0b5fe-3703-4aa8-9ac3-129bea317710",
   "metadata": {},
   "source": [
    "- During training, we seek parameter values $φ$ that minimize the loss and hence map the training inputs to the outputs as closely as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf174b-b9d9-4dfb-aaa0-5a7b6d50081f",
   "metadata": {},
   "source": [
    "### 1.1 Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3857d7d5-d10c-4ad4-8f7c-2a4b16d432c4",
   "metadata": {},
   "source": [
    "- Consider a model $f[x, φ]$ with parameters $φ$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d82a13-21f1-4a9f-8295-93e2986c8fdc",
   "metadata": {},
   "source": [
    "- The model computes a **conditional probability distribution** $Pr(y|x)$ over possible outputs $y$ given input $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d8775-e45e-48af-9ccc-bab3eb8ca3b1",
   "metadata": {},
   "source": [
    "- The loss encourages each training output $y_{i}$ to have a high probability under the distribution $Pr(y_{i}|x_{i})$ computed from the corresponding input $x_{i}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea778224-5fcd-4ec6-968d-6b3dc7b3dcd6",
   "metadata": {},
   "source": [
    "<img src=\"img/pic12.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787799d6-5a72-4948-bc1c-b776f5e3e91d",
   "metadata": {},
   "source": [
    "**How a model $f[x, φ]$ can be adapted to compute a probability distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4096b3e-718f-48ce-a15c-0a351ce10c69",
   "metadata": {},
   "source": [
    "- The model can computes different distribution parameters $θ_{i} = f[x_{i}, φ]$ for each training input $x_{i}$.\n",
    "- Each observed training output $y_{i}$ should have high probability under its corresponding distribution $Pr(y_{i}|θ_{i})$. \n",
    "- So, we choose the model parameters $φ$ so that they maximize the combined probability across all $I$ training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a55ba-b095-4e4b-ac89-8aa8a66affc0",
   "metadata": {},
   "source": [
    "<img src=\"img/pic13.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7172ecfb-c5b1-4abe-b126-4ff7c151b685",
   "metadata": {},
   "source": [
    "This is known as **the maximum likelihood criterion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382eec8c-1d1e-4587-b3a8-9f9d966f3ffc",
   "metadata": {},
   "source": [
    "<img src=\"img/pic14.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216bd30-4862-4f44-9ed2-08ab02c5a725",
   "metadata": {},
   "source": [
    "we assume the data are **independent and identically distributed (i.i.d.).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc508ee7-c1e5-48e0-bd3f-c8cf8ac2e42d",
   "metadata": {},
   "source": [
    "### 1.2 Maximizing log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f47a9f-a758-4825-8167-2cc32e60c676",
   "metadata": {},
   "source": [
    "- The maximum likelihood results in very small values. Instead, we can maximize the logarithm of the likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e0510-27f3-4ebd-927d-89b012fdb3ec",
   "metadata": {},
   "source": [
    "<img src=\"img/pic15.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f57aa9b-905d-40e3-98be-e7e48187de9d",
   "metadata": {},
   "source": [
    "- To convert the maximum log-likelihood criterion to a minimization problem, we multiply by minus one, which gives us **the negative log-likelihood criterion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebc765-f0b3-4991-9990-1a2a05899c33",
   "metadata": {},
   "source": [
    "<img src=\"img/pic16.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff660e6-96ad-4b6c-a716-e5b1c315c535",
   "metadata": {},
   "source": [
    "### Example 1: univariate regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef17a0-b743-49d8-9b0f-141a363a4e70",
   "metadata": {},
   "source": [
    "- We choose the univariate normal distribution, which is defined over $y \\in R$. \n",
    "- This has two parameters (mean $μ$ and variance $σ^{2}$) and has a probability density function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c903f1-90d1-40a2-9af8-a5583e48044d",
   "metadata": {},
   "source": [
    "<img src=\"img/pic17.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ddd407-8280-4ce3-a4dc-2585e0db977d",
   "metadata": {},
   "source": [
    "- we set the machine learning model $f[x, φ]$ to compute one or more of the parameters of this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399cccf4-fceb-4b4d-b1e0-967db7acd0d6",
   "metadata": {},
   "source": [
    "<img src=\"img/pic18.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a5658-6c84-484e-91c2-db375dd9fc8e",
   "metadata": {},
   "source": [
    "- we choose a loss function $L[φ]$ based on the negative log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5da869-5473-4b62-a4d0-efb6ec9254c3",
   "metadata": {},
   "source": [
    "<img src=\"img/pic19.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c866d-dd23-41d6-a8e8-e664d22a13bd",
   "metadata": {},
   "source": [
    "### Least squares loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abb4d89-5e32-4eb3-9c50-852355538321",
   "metadata": {},
   "source": [
    "We perform some algebraic manipulations on the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76274072-b7e5-4c5b-b12d-2e9635ac2c99",
   "metadata": {},
   "source": [
    "<img src=\"img/pic20.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54541f03-0b2e-464b-bfb6-adc4110f3364",
   "metadata": {},
   "source": [
    "- The result of these manipulations is the least squares loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb2f6f-8fb0-4be1-a6fd-d74b88e446b1",
   "metadata": {},
   "source": [
    "## Example 2: binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5011fc02-2c6f-4ed0-a3ee-3007415626ad",
   "metadata": {},
   "source": [
    "In binary classification, the goal is to assign the data $x$ to one of two discrete classes $y \\in \\{0, 1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d771f16-0fc4-417e-8c20-847f0135b01a",
   "metadata": {},
   "source": [
    "- We choose a probability distribution over the output space $y \\in \\{0, 1\\}$.\n",
    "- A suitable choice is the Bernoulli distribution, which is defined on the domain $\\{0, 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e0e188-1d25-4b6f-b906-e11fd872c652",
   "metadata": {},
   "source": [
    "<img src=\"img/pic21.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ac020-21d4-4ab2-bad3-f521e192e126",
   "metadata": {},
   "source": [
    "- We set the machine learning model $f[x,φ]$ to predict the single distribution parameter $λ$.\n",
    "- However, $λ$ can only take values in the range $[0, 1]$\n",
    "- So, we pass the network output through a function that maps the real numbers $R$ to $[0,1]$.\n",
    "- A suitable function is the logistic sigmoid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d90e3e4-e2e2-451f-b19c-08c2e765a557",
   "metadata": {},
   "source": [
    "<img src=\"img/pic22.png\" width=200 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8351339-5112-4689-b2fe-18829a463f2f",
   "metadata": {},
   "source": [
    "The likelihood is now: $P r(y|x) = (1 − sig[f[x, φ]])^{1−y} · sig[f[x, φ]]^{y}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c849e-3118-415d-a0b5-5d2b61fb674a",
   "metadata": {},
   "source": [
    "- The loss function is the negative log-likelihood of the training set:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54eac6-f571-4c68-b527-aff0e35ef94a",
   "metadata": {},
   "source": [
    "<img src=\"img/pic23.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9837737c-f52a-4192-bd5f-e18cdeabdf4d",
   "metadata": {},
   "source": [
    "- this is also known as **the binary cross-entropy loss**\n",
    "- When we perform inference, we may want a point estimate of $y$,\n",
    "- so we set $y = 1$ if $λ > 0.5$ and $y = 0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ffe69-ceff-43ff-b77f-6c9006d65477",
   "metadata": {},
   "source": [
    "<img src=\"img/pic24.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5dff4-a372-402a-a418-6eb21247e3ba",
   "metadata": {},
   "source": [
    "### Example 3: multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f9dae2-e3af-44dd-95df-b1a90fdf05d0",
   "metadata": {},
   "source": [
    "- We have $y \\in {1, 2, . . . , K}$, so we choose **the categorical distribution**\n",
    "\n",
    "$Pr(y = k) = λ_{k}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16945aa-2d37-4be8-90fe-f55c07a61a1c",
   "metadata": {},
   "source": [
    "- The parameters are constrained to take values between zero and one, and they must collectively sum to one to ensure a valid probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f07869-14aa-46b6-b0f7-29d0dc8cccca",
   "metadata": {},
   "source": [
    "- So, we pass the $K$ outputs of the network through a function that ensures these constraints are respected.\n",
    "- A suitable choice is **the softmax function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff429a1-1bfa-47a2-957e-4ba3067fff44",
   "metadata": {},
   "source": [
    "<img src=\"img/pic25.png\" width=250 height=250 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da99526-5baa-4a5b-b291-36bb92ba6bc4",
   "metadata": {},
   "source": [
    "The loss function is the negative log-likelihood of the training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17e901-b61f-44d8-9f85-e8000ecd8cce",
   "metadata": {},
   "source": [
    "<img src=\"img/pic26.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a4b5c-a926-497c-9221-a2129672ecc5",
   "metadata": {},
   "source": [
    "- this is known as the **multiclass cross-entropy loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df31a7f-2602-4291-ac0e-2c700ca0941f",
   "metadata": {},
   "source": [
    "## 2. Fitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22623b78-1c41-4b90-8bbc-7197ca539d89",
   "metadata": {},
   "source": [
    "- how to find the parameter values that minimize the loss\n",
    "- known as learning the network’s parameters or simply as training or fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aacc87-813d-4482-b524-b00d94c99fb7",
   "metadata": {},
   "source": [
    "**The process is to choose initial parameter values and then iterate the following two steps:**\n",
    "- (i) compute the derivatives (gradients) of the loss with respect to the parameters, and \n",
    "- (ii) adjust the parameters based on the gradients to decrease the loss. \n",
    "- After many iterations, we hope to reach the overall minimum of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f051d-2d12-40b2-b752-8e4091bafc1a",
   "metadata": {},
   "source": [
    "### 2.1 Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10c1f3-66a9-45e3-a247-b66ff359465b",
   "metadata": {},
   "source": [
    "- The goal of an optimization algorithm is to find parameters $φˆ$ that minimize the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8301365-299f-48c4-a00a-6c9304f6cba2",
   "metadata": {},
   "source": [
    "<img src=\"img/pic27.png\" width=200 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd41dad-a9a2-436d-a1f8-a99696280f61",
   "metadata": {},
   "source": [
    "Gradient descent starts with initial parameters $φ = [φ_{0},φ_{1},...,φ_{N}]^{T}$ and iterates two steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c09f3-9004-419e-a991-9cd1386c4d19",
   "metadata": {},
   "source": [
    "- Compute the derivatives of the loss with respect to the parameters:\n",
    "\n",
    "<img src=\"img/pic28.png\" width=150 height=150 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a2e2fa-d6c4-4750-abbc-49dca3d97a6c",
   "metadata": {},
   "source": [
    "- Update the parameters according to the rule:\n",
    "\n",
    "<img src=\"img/pic29.png\" width=150 height=150 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be687aba-1fd7-42b1-b5e4-300f664c38dc",
   "metadata": {},
   "source": [
    "### Linear regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf06e34-955f-44c1-872e-4190babaa3b2",
   "metadata": {},
   "source": [
    "<img src=\"img/pic30.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344c5c5-91e4-4069-a5b3-1bc2b45e7074",
   "metadata": {},
   "source": [
    "- The derivative of the loss function with respect to the parameters can be decomposed into the sum of the derivatives of the individual contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65feb68e-369a-425c-8e43-00f1e7c72e8b",
   "metadata": {},
   "source": [
    "<img src=\"img/pic31.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951163b6-e7de-4576-89d5-f526584825d9",
   "metadata": {},
   "source": [
    "### Local minima and saddle points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2babdcc0-b7d3-44e8-bb53-1c2a401c133a",
   "metadata": {},
   "source": [
    "- Loss functions for linear regression problems always have a single well-defined **global minimum**. More formally, they are **convex**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b7878-e2e6-46fc-a507-e83b85eacc5a",
   "metadata": {},
   "source": [
    "- However, loss functions for most nonlinear models, including both shallow and deep networks, are **non-convex**, meaning there are numerous local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06011ab6-7f28-45bd-85f0-3e888e4dbbe2",
   "metadata": {},
   "source": [
    "- Also, the loss function may contain **saddle points**. Here, the gradient is zero, but the function increases in some directions and decreases in others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361f831-3ff6-4191-9b30-4b3a8d516060",
   "metadata": {},
   "source": [
    "How could we deal with this problem?\n",
    "- either (i) exhaustively searching the parameter space or \n",
    "- (ii) repeatedly starting gradient descent from different positions and choosing the result with the lowest loss.\n",
    "\n",
    "But this is not practical!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba03c7ce-a145-4aa2-a801-c99bf9e048a8",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e7c75-618d-4b99-af47-701992a5e1ed",
   "metadata": {},
   "source": [
    "- Stochastic gradient descent (SGD) attempts to remedy this problem by adding some noise to the gradient at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed34eb-f217-44d9-a3fd-4de30cc7747a",
   "metadata": {},
   "source": [
    "<img src=\"img/pic32.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54ec48-5342-4786-9479-50c21a479ee3",
   "metadata": {},
   "source": [
    "### Batches and epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc4c95-5866-4b6e-b9ac-2e09ceeb9292",
   "metadata": {},
   "source": [
    "At each iteration, the algorithm chooses a random subset of the training data and computes the gradient from these examples alone. \n",
    "\n",
    "This subset is known as a minibatch or batch for short. The update rule for the model parameters $φ_{t}$ at iteration $t$ is hence:\n",
    "\n",
    "<img src=\"img/pic33.png\" width=200 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e641a-e669-4211-b390-e292f531e4b3",
   "metadata": {},
   "source": [
    "- The term $α$ is the learning rate, and together with the gradient magnitude, determines the distance moved at each iteration.\n",
    "- A single pass through the entire training dataset is referred to as **an epoch**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6ffa5-4a08-493f-ba0a-997843fb7dbb",
   "metadata": {},
   "source": [
    "#### Properties of stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d123b-3251-4c17-acfc-6396918ca565",
   "metadata": {},
   "source": [
    "1- Although it adds noise to the trajectory, it still improves the fit to a subset of the data at each iteration.\n",
    "\n",
    "2- The training examples all contribute equally\n",
    "\n",
    "3- It is less computationally expensive\n",
    "\n",
    "4- It can (in principle) escape local minima\n",
    "\n",
    "5- It reduces the chances of getting stuck near saddle points\n",
    "\n",
    "6- There is some evidence that SGD finds parameters for neural networks that cause them to generalize well in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbaffbf-6d69-4db8-9da9-a7f3e44906d7",
   "metadata": {},
   "source": [
    "### 3. Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e3544-f837-407f-8d3f-dc177283edab",
   "metadata": {},
   "source": [
    "- We can update the parameters with a weighted combination of the gradient computed from the current batch and the direction moved in the previous step:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842346f-4ddd-4e62-8982-d12ba7e70aa5",
   "metadata": {},
   "source": [
    "<img src=\"img/pic34.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ed21b-e589-4f3d-a55f-c446af015111",
   "metadata": {},
   "source": [
    "where $m_{t}$ is the momentum and $β \\in [0, 1)$ controls the degree to which the gradient is smoothed over time.\n",
    "\n",
    "-- The overall effect is a smoother trajectory and reduced oscillatory behavior in valleys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330bdf4f-c7c0-4079-a107-ffcaf1e7fcfe",
   "metadata": {},
   "source": [
    "### Nesterov accelerated momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12348a53-a2d6-42a3-a944-e7cf05cd39bb",
   "metadata": {},
   "source": [
    "- The momentum term can be considered **a prediction of where the SGD algorithm will move next**. \n",
    "- Nesterov accelerated momentum **computes the gradients at this predicted point** rather than at the current point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4303ce-169e-47b6-ac1f-5aff34f3464e",
   "metadata": {},
   "source": [
    "<img src=\"img/pic36.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e14754-0f15-4198-8037-d5e1b82bfe3e",
   "metadata": {},
   "source": [
    "- See that the gradients are evaluated at $φ_{t} − αβ · m_{t}$\n",
    "- We can see this as the gradient term now corrects the path provided by momentum alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340e791-4c0b-4fa0-a474-e5d4b670b574",
   "metadata": {},
   "source": [
    "<img src=\"img/pic35.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7851bbc-fd75-4fcb-841d-33eec6fc9854",
   "metadata": {},
   "source": [
    "### Adaptive moment estimation (Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26aa58-7357-4d51-bcfa-a3b711129ed1",
   "metadata": {},
   "source": [
    "- Gradient descent with a fixed step size has the following undesirable property:\n",
    "    - it makes large adjustments to parameters associated with large gradients\n",
    "    - and small adjustments to parameters associated with small gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ce80dd-d27e-4e4e-95c6-eb93fb9510ed",
   "metadata": {},
   "source": [
    "- When the gradient of the loss surface is much steeper in one direction than another, it is diﬀicult to choose a learning rate that\n",
    "    - (i) makes good progress in both directions and\n",
    "    - (ii) is stable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ccb8e-3cd2-4741-b8de-a08807278191",
   "metadata": {},
   "source": [
    "- A straightforward approach is to normalize the gradients so that we move a fixed distance (governed by the learning rate) in each direction\n",
    "- To do this, we first measure the gradient $m_{t+1}$ and the pointwise squared gradient $v_{t+1}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15442963-ea5e-496f-800b-bfafaac998dc",
   "metadata": {},
   "source": [
    "<img src=\"img/pic36.png\" width=350 height=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236398b3-80b8-4438-a3ea-bcda1e123fa4",
   "metadata": {},
   "source": [
    "- Adam, takes this idea and adds momentum to both the estimate of the gradient and the squared gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b671e55-c4f5-4427-a744-5cdd09ae8b51",
   "metadata": {},
   "source": [
    "<img src=\"img/pic39.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b44bc-cb89-4df9-9e30-84d3e57db4e7",
   "metadata": {},
   "source": [
    "- Using momentum is equivalent to taking a weighted average over the history of each of these statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a67451b-7963-437c-afbb-a9bf4b4d729a",
   "metadata": {},
   "source": [
    "<img src=\"img/pic40.png\" width=200 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5970fdb7-1022-4033-9ab3-ae51cc431113",
   "metadata": {},
   "source": [
    "<img src=\"img/pic41.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c9885-6c40-4a9c-bdb8-4139445e889c",
   "metadata": {},
   "source": [
    "<img src=\"img/pic38.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04d514-f9c1-4498-bf9e-3b9b7a4eadfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
